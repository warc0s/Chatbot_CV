# ü§ñ ChatBot CV: RAG vs Ventana de Contexto

![Banner](https://github.com/warc0s/Chatbot_CV/blob/main/extra/banner.png?raw=true)

## üìã √çndice
- [Descripci√≥n General](#-descripci√≥n-general)
- [Arquitectura Dual: RAG vs Ventana de Contexto](#-arquitectura-dual-rag-vs-ventana-de-contexto)
  - [¬øQu√© es RAG?](#qu√©-es-rag)
  - [¬øQu√© es una Gran Ventana de Contexto?](#qu√©-es-una-gran-ventana-de-contexto)
  - [Comparaci√≥n de Enfoques](#comparaci√≥n-de-enfoques)
- [Implementaciones T√©cnicas](#-implementaciones-t√©cnicas)
  - [Enfoque 1: LlamaIndex + Llama 3.3 70B](#enfoque-1-llamaindex--llama-33-70b)
  - [Enfoque 2: Gemini Flash con Contexto Completo](#enfoque-2-gemini-flash-con-contexto-completo)
- [Sistema Inteligente de Clasificaci√≥n](#-sistema-inteligente-de-clasificaci√≥n)
- [Arquitectura y Optimizaci√≥n](#%E2%9A%99%EF%B8%8F-arquitectura-y-optimizaci√≥n)
- [Frontend y Experiencia de Usuario](#-frontend-y-experiencia-de-usuario)
- [Tecnolog√≠as y Frameworks](#%F0%9F%9B%A0%EF%B8%8F-tecnolog√≠as-y-frameworks)
- [Instalaci√≥n y Uso](#-instalaci√≥n-y-uso)
- [Contribuci√≥n](#-contribuci√≥n)
- [Licencia](#-licencia)

## üìã Descripci√≥n General

Este proyecto implementa y compara dos arquitecturas avanzadas de chatbot aplicadas a la consulta de informaci√≥n curricular. El objetivo es analizar las diferencias en rendimiento, precisi√≥n y experiencia de usuario entre:

1. **Generaci√≥n Aumentada por Recuperaci√≥n (RAG)** con Llama 3.3 y LlamaIndex
2. **Modelo con Gran Ventana de Contexto** usando Gemini Flash

Ambos chatbots proporcionan respuestas detalladas sobre mi experiencia profesional, habilidades y formaci√≥n, pero utilizan mecanismos fundamentalmente diferentes para acceder y procesar la informaci√≥n. Este proyecto ofrece as√≠ un interesante caso de estudio sobre los trade-offs entre diferentes arquitecturas de IA conversacional.

## üß† Arquitectura Dual: RAG vs Ventana de Contexto

### ¬øQu√© es RAG?

**RAG (Retrieval Augmented Generation)** es una arquitectura que combina:

1. **Recuperaci√≥n (Retrieval)**: El sistema busca en tiempo real informaci√≥n relevante en una base de conocimientos (en este caso, mi CV indexado con embeddings vectoriales).

2. **Generaci√≥n Aumentada**: El modelo de lenguaje (LLM) utiliza la informaci√≥n recuperada para generar respuestas m√°s precisas y fundamentadas en datos concretos.

RAG funciona como un sistema de dos etapas:
- Primero convierte la pregunta del usuario en una consulta vectorial
- Luego busca los fragmentos m√°s relevantes del CV mediante similitud sem√°ntica
- Finalmente, proporciona estos fragmentos como contexto adicional al LLM para generar la respuesta

### ¬øQu√© es una Gran Ventana de Contexto?

La **ventana de contexto** determina cu√°nta informaci√≥n puede "ver" un modelo en una sola conversaci√≥n. Modelos como Gemini Flash poseen una ventana de contexto excepcionalmente grande (hasta 1 mill√≥n de tokens), lo que permite:

- Cargar documentos completos (como mi CV) directamente en el prompt
- Mantener toda la conversaci√≥n previa dentro del contexto
- Responder bas√°ndose en una comprensi√≥n hol√≠stica del documento sin necesidad de recuperaci√≥n externa

En este enfoque, el modelo tiene acceso constante a toda la informaci√≥n, similar a como un humano tendr√≠a un documento completo frente a √©l durante toda la conversaci√≥n.

### Comparaci√≥n de Enfoques

| Aspecto | Enfoque RAG | Enfoque Gran Ventana |
|---------|-------------|----------------------|
| **Precisi√≥n** | Precisi√≥n variable dependiente de la calidad de recuperaci√≥n y estrategia de chunking | Comprensi√≥n completa del documento con visi√≥n global |
| **Flexibilidad** | Puede acceder a grandes vol√∫menes de informaci√≥n externa | Limitado estrictamente a lo que se carg√≥ en la ventana de contexto |
| **Consumo de recursos** | Uso eficiente de tokens en el LLM, pero requiere infraestructura para vectorizaci√≥n e indexaci√≥n | Consumo elevado de tokens en cada consulta independientemente de su complejidad |
| **Velocidad** | Mayor latencia debido al proceso de b√∫squeda + generaci√≥n | Respuesta m√°s directa y r√°pida al tener toda la informaci√≥n pre-cargada |
| **Escalabilidad** | Puede escalar pr√°cticamente sin l√≠mites en volumen de informaci√≥n | Estrictamente limitado por el tama√±o m√°ximo de la ventana de contexto |
| **Dependencia de chunking** | Altamente dependiente de la estrategia de segmentaci√≥n; chunks mal dise√±ados pueden omitir informaci√≥n relevante | No requiere chunking, eliminando este posible problema |
| **Adaptabilidad** | Permite actualizar la base de conocimientos sin reentrenamiento | Requiere regenerar el prompt completo para actualizar informaci√≥n |

## üíª Implementaciones T√©cnicas

### Enfoque 1: LlamaIndex + Llama 3.3 70B

Esta implementaci√≥n utiliza:

- **Modelo base**: Llama 3.3 70B de Groq
- **Framework RAG**: LlamaIndex para indexaci√≥n y recuperaci√≥n
- **Embeddings**: BGE-M3 (BAAI) a trav√©s de DeepInfra para crear representaciones vectoriales del CV
- **Almacenamiento**: √çndice vectorial persistente para optimizar rendimiento
- **Backend**: Flask con arquitectura RESTful
- **Manejo de sesiones**: Sistema de historial de conversaci√≥n con contexto persistente

El flujo de funcionamiento:
1. La consulta del usuario se vectoriza
2. Se recuperan los fragmentos m√°s relevantes del CV
3. Se combina la consulta, fragmentos relevantes e historial de conversaci√≥n
4. Se genera una respuesta contextualizada con el modelo Llama 3.3 70B

### Enfoque 2: Gemini Flash con Contexto Completo

Esta versi√≥n aprovecha la enorme ventana de contexto:

- **Modelo principal**: Gemini 2.0 Flash (1M tokens de contexto)
- **Modelo alternativo**: Gemini 2.0 Flash-Lite (para consultas sencillas)
- **Prompt Engineering**: CV completo incluido en el contexto inicial
- **Clasificador**: BERT en espa√±ol para categorizar consultas
- **Integraci√≥n**: LiteLLM para gesti√≥n de API y solicitudes
- **Gesti√≥n de memoria**: Optimizaci√≥n del historial de conversaci√≥n

Funcionamiento:
1. El CV completo se carga en el contexto del modelo (System Prompt)
2. BERT clasifica la consulta por complejidad
3. Se selecciona el modelo apropiado (Flash o Flash-Lite)
4. Se genera la respuesta con todo el contexto disponible

## üîç Sistema Inteligente de Clasificaci√≥n

El sistema BERT implementado en la versi√≥n Gemini clasifica las consultas en tres categor√≠as:

1. **Consultas simples** (0): Preguntas directas sobre informaci√≥n b√°sica del CV
   - Ejemplo: "¬øD√≥nde estudi√≥ Marcos?"
   - Procesadas por: Gemini Flash-Lite (m√°s econ√≥mico y r√°pido)

2. **Consultas complejas** (1): Preguntas que requieren an√°lisis, razonamiento o s√≠ntesis
   - Ejemplo: "¬øC√≥mo se complementan sus habilidades t√©cnicas y de gesti√≥n?"
   - Procesadas por: Gemini Flash (m√°s potente y con mayor capacidad de razonamiento)

3. **Consultas fuera de √°mbito** (2): Preguntas no relacionadas con el CV
   - Ejemplo: "¬øC√≥mo funciona la fusi√≥n nuclear?"
   - Resultado: Bloqueadas para evitar uso inadecuado del sistema

Esta clasificaci√≥n inteligente permite:
- Optimizar costos dirigiendo consultas al modelo m√°s adecuado
- Mejorar tiempos de respuesta para preguntas simples
- Mantener el enfoque del chatbot en su prop√≥sito espec√≠fico
- Prevenir posibles abusos o desv√≠os del tema principal

## ‚öôÔ∏è Arquitectura y Optimizaci√≥n

Ambas implementaciones comparten elementos arquitect√≥nicos clave:

- **Separaci√≥n backend/frontend**: API REST desacoplada de la interfaz de usuario
- **Gesti√≥n de sesiones**: Identificadores √∫nicos para mantener conversaciones coherentes
- **Persistencia**: Almacenamiento eficiente de √≠ndices (en LlamaIndex) y sesiones
- **Concurrencia**: Control de solicitudes simult√°neas para evitar sobrecarga
- **Caducidad**: Limpieza autom√°tica de sesiones inactivas
- **Prompt Engineering**: Instrucciones detalladas para garantizar respuestas profesionales y precisas

Optimizaciones espec√≠ficas:
- **RAG**: Chunking personalizado, caching de embeddings
- **Contexto**: Gesti√≥n eficiente de tokens, historial selectivo, plantillas de prompt optimizadas

## üé® Frontend y Experiencia de Usuario

Ambas versiones presentan interfaces modernas y responsivas:

- **UI Adaptativa**: Dise√±o responsive con Tailwind CSS
- **Animaciones**: Transiciones suaves para mejorar la experiencia
- **Mensajes de Estado**: Indicadores durante el procesamiento con etapas descriptivas
- **Markdown**: Soporte completo para respuestas formateadas con listas, enlaces, etc.
- **Historial**: Persistencia de conversaciones entre sesiones
- **Accesibilidad**: Contraste adecuado y estructuras sem√°nticas
- **Mobile-first**: Optimizado para dispositivos m√≥viles y escritorio

## üõ†Ô∏è Tecnolog√≠as y Frameworks

### Modelos y APIs
- **Llama 3.3 70B**: A trav√©s de Groq API
- **Gemini 2.0 Flash/Flash-Lite**: Google AI API
- **BGE-M3**: Embeddings v√≠a DeepInfra
- **BERT**: Fine-tuned para clasificaci√≥n de intenciones

### Frameworks y Librer√≠as
- **LlamaIndex**: Framework para implementaci√≥n RAG
- **Flask**: Backend ligero y eficiente
- **Tailwind CSS**: Estilizado moderno y responsive
- **LiteLLM**: Abstracci√≥n para APIs de cualquier proveedor de LLM
- **Marcado/Renderizado**: Marked.js para procesamiento de markdown
- **LocalStorage**: Persistencia en el lado cliente

## üì¶ Instalaci√≥n y Uso

### Requisitos
- Python 3.10+
- API Keys: Groq, Google AI, DeepInfra

### Configuraci√≥n del Backend
*Instrucciones de instalaci√≥n:*

*# Clonar el repositorio*
*git clone https://github.com/warc0s/Chatbot_CV.git*
*cd Chatbot_CV*

*# Crear entorno virtual*
*python -m venv venv*
*source venv/bin/activate  # En Windows: venv\Scripts\activate*

*# Instalar dependencias*
*pip install -r requirements.txt*

*# Configurar variables de entorno*
*# Editar .env con tus API keys*

*# Iniciar servidor*
*python app.py*

### Uso del Frontend
El frontend estar√° disponible en `http://localhost:5000` tras iniciar el servidor.

## üë• Contribuci√≥n

Las contribuciones son bienvenidas. Para contribuir:

1. Haz un fork del repositorio
2. Crea una rama para tu caracter√≠stica (`git checkout -b feature/nueva-caracteristica`)
3. Realiza tus cambios y haz commit (`git commit -am 'A√±adir nueva caracter√≠stica'`)
4. Sube los cambios (`git push origin feature/nueva-caracteristica`)
5. Abre un Pull Request

## üìú Licencia

Este proyecto est√° licenciado bajo Apache License 2.0. Consulta el archivo [LICENSE](LICENSE) para m√°s detalles.
